{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cff789b-a218-4d39-992e-c423b68bef8a",
   "metadata": {},
   "source": [
    "This notebook will demonstrate the process I went through to run `neuralbeagle14-7b` on my laptop's 8GB GPU in Windows, as seen in [my LinkedIn post from January 27th, 2024](https://www.linkedin.com/posts/jsundance_free-local-private-ai-on-my-laptop-thanks-activity-7157117360728862720-MWxn?utm_source=share&utm_medium=member_desktop). It pulls heavily from [this LangChain documentation](https://python.langchain.com/docs/integrations/llms/llamacpp).\n",
    "\n",
    "I was able to use `llama-cpp-python` _without_ my GPU, and it took me a couple installs before it was really loading all of the layers onto the GPU. It was still fast without the GPU, but that's not the point. ;)\n",
    "\n",
    "I'm using an NVIDIA RTX A4000 laptop GPU. I will be compiling `llama-cpp-python` instead of using the \"usual\" `pip install` because I _think_ this is a more reliable method. I will be using the cuBLAS backend, but you can other backends for AMD or Apple or whatever (more or this later).\n",
    "\n",
    "I will also describe an issue I had with a missing dll, and offer troubleshooting advice for that.\n",
    "\n",
    "Joshua Bailey #LearningInPublic January 28, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eabf3c-27e7-43dd-8017-f3f8dd4f961a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prerequisites (and gotchas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491caa09-5dea-48c1-bb51-2ed2c1f5850f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NVIDIA stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183c5bc-029c-4394-9bdb-783b21f9745f",
   "metadata": {},
   "source": [
    "- [NVIDIA driver](https://www.nvidia.com/download/index.aspx)\n",
    "- [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778907f6-ea8d-404f-83bc-b57784576c39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Microsoft Visual Studio stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3c244-2c0c-4d48-8b96-25d8c729c471",
   "metadata": {},
   "source": [
    "From [the LangChain documentation](https://python.langchain.com/docs/integrations/llms/llamacpp):\n",
    "\n",
    "- Visual Studio Community (make sure you install this with the following settings)\n",
    "  - Desktop development with C++\n",
    "  - Python development\n",
    "  - Linux embedded development with C++\n",
    "\n",
    "_side note_: I installed this stuff a while ago along with `cudnn` for ArcGIS deep learning, and I don't think I included the Linux embedded development thing (maybe), which is probably why I had the dll trouble I'll describe later. ;)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7373ed17-dad5-4530-a1ca-2b95774e1e3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check `nvidia-smi` and `nvcc --version`\n",
    "\n",
    "If either of these commands don't work, you'll have trouble.\n",
    "Install NVIDIA driver and CUDA toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779f4d54-fd62-4d5f-ae05-f3c8d2b778cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 28 17:29:46 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.79                 Driver Version: 537.79       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000 Laptop GPU  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   55C    P8              16W / 110W |      0MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c6657c-79c6-4335-868b-d00e64bf3a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89ed97-c1e3-4923-a7a6-16ebd9ff8f17",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Python stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082553d6-a620-46f0-99a2-690684436feb",
   "metadata": {},
   "source": [
    "First of all, I highly recommend using an environment management tool like `conda` to manage your environments-- and never tinker around in the base environment. That way, when your package versions get messed up or whatever, you can just start fresh. ;)\n",
    "\n",
    "A common approach is to install [anaconda](https://anaconda.org/). \n",
    "\n",
    "Assuming you have `conda`, create a new Python environment. At the time of writing, version constraints meant that Python 3.12 was not supported, so:\n",
    "\n",
    "```cmd\n",
    "conda create -n llama-cpp-python python=3.11\n",
    "conda activate llama-cpp-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0b462-50b2-4803-8bd2-8f236531a4d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `torch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c21f5e-1787-4159-8bec-03e2e449b0a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "You have to install `torch` before installing `llama-cpp-python`. I think if you just `pip install torch` then you get the cpu-only version.\n",
    "\n",
    "So assuming you'll be using CUDA 11.8, based on [the pytorch documentation](https://pytorch.org/get-started/locally/), run:\n",
    "\n",
    "```cmd\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "(12.1 is available at `/whl/cu121` but 12.2 is apparently not supported yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa50379-f0a4-4a7d-b4a0-88714cd2d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a32f5a-365d-4aa4-89b5-90c0a4cf7050",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Compiling and installing `llama-cpp-python`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36306695-bbc3-4b4d-a1d2-a7973b5a3eb1",
   "metadata": {},
   "source": [
    "```markdown\n",
    "There are different options on how to install the llama-cpp package:\n",
    "\n",
    "- CPU usage\n",
    "- CPU + GPU (using one of many BLAS backends)\n",
    "- Metal GPU (MacOS with Apple Silicon Chip)\n",
    "```\n",
    "(from https://python.langchain.com/docs/integrations/llms/llamacpp)\n",
    "\n",
    "In Windows with CUBLAS:\n",
    "\n",
    "```cmd\n",
    "set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "set FORCE_CMAKE=1\n",
    "pip install -v llama-cpp-python\n",
    "# or if you've already installed it and need to try again (or you just wanna be extra careful I guess?):\n",
    "# pip install -v --upgrade --force-reinstall --no-cache-dir llama-cpp-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c57974-3129-4c49-b06a-362ee34e153d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `No CUDA toolset found`? Check for `Nvda.Build.CudaTasks.v*.*.dll`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdb49b-775f-4451-8152-16d257aa8b06",
   "metadata": {},
   "source": [
    "If you try to compile `llama-cpp-python` and get an error message like\n",
    "```text\n",
    "[...]\n",
    "CMake Error at [...]\n",
    "No CUDA toolset found.\n",
    "[...]\n",
    "*** CMake configuration failed.\n",
    "[end of output]\n",
    "```\n",
    "\n",
    "Then take a look at [this GitHub issue comment](https://github.com/NVlabs/tiny-cuda-nn/issues/164#issuecomment-1280749170) and possibly use the following code to help find your problem.\n",
    "\n",
    "If everything's good, the code below will print stuff and not raise any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82432156-34c1-4a6c-a8ec-cb3bdbe02603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\extras\\visual_studio_integration\\MSBuildExtensions\\Nvda.Build.CudaTasks.v11.4.dll\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\extras\\visual_studio_integration\\MSBuildExtensions\\Nvda.Build.CudaTasks.v11.8.dll \n",
      "\n",
      "Highest version: 11.8\n",
      "\n",
      "Build customizations dir: C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\n",
      "\n",
      "Checking for files:\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 11.8.xml\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 11.8.props\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\Nvda.Build.CudaTasks.v11.8.dll\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\\CUDA 11.8.targets \n",
      "\n",
      "All good\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "def version_files(version: str) -> set[str]:\n",
    "    return {\n",
    "        f\"CUDA {version}.props\",\n",
    "        f\"CUDA {version}.targets\",\n",
    "        f\"CUDA {version}.xml\",\n",
    "        f\"Nvda.Build.CudaTasks.v{version}.dll\",\n",
    "    }\n",
    "\n",
    "dll_pat = re.compile(r\"^Nvda.Build.CudaTasks.v(?P<major>\\d{2})\\.(?P<minor>\\d)\\.dll$\")\n",
    "\n",
    "nvidia_cuda_glob = f\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\**\\\\extras\\\\visual_studio_integration\\\\MSBuildExtensions\\\\Nvda.Build.CudaTasks.v*.*.dll\"\n",
    "nvidia_cuda_files = glob(nvidia_cuda_glob, recursive=True)\n",
    "print(f\"Found files:\")\n",
    "print(\"\\n\".join(nvidia_cuda_files), \"\\n\")\n",
    "\n",
    "if not nvidia_cuda_files:\n",
    "    raise RuntimeError()\n",
    "\n",
    "basenames = (os.path.basename(f) for f in nvidia_cuda_files)\n",
    "matches = (dll_pat.match(bn) for bn in basenames)\n",
    "groups = (match.groupdict() for match in matches if match)\n",
    "sorted_versions = sorted(groups, key=lambda x: (int(x['major']), int(x['minor'])))\n",
    "highest_version = sorted_versions[-1]\n",
    "highest_str = highest_version['major'] + '.' + highest_version['minor']\n",
    "highest_files = version_files(highest_str)\n",
    "\n",
    "print(f\"Highest version: {highest_str}\\n\")\n",
    "\n",
    "bc_dirs = glob(\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\*\\\\BuildTools\\\\MSBuild\\\\Microsoft\\\\VC\\\\v*\\\\BuildCustomizations\", recursive=True)\n",
    "if len(bc_dirs) != 1:\n",
    "    print(\"Only expected to find one directory lol\")\n",
    "    print(bc_dirs)\n",
    "    raise RuntimeError()\n",
    "bc_dir = bc_dirs[0]\n",
    "\n",
    "print(f\"Build customizations dir: {bc_dir}\\n\")\n",
    "\n",
    "expected_files = [os.path.join(bc_dir, file) for file in highest_files]\n",
    "print(\"Checking for files:\")\n",
    "print(\"\\n\".join(expected_files), \"\\n\")\n",
    "\n",
    "for file in highest_files:\n",
    "    expected_file = os.path.join(bc_dir, file)\n",
    "    if not os.path.exists(expected_file):\n",
    "        raise FileNotFoundError(expected_file)\n",
    "\n",
    "print(\"All good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25efc524-b3a5-4884-ba9d-067e570c8046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Using `neuralbeagle14-7b` in `langchain`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6562ca-2771-4514-aab9-ed0079a6e99b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Enable LangSmith logging (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79df914-5ae5-439b-b73d-8e257b827330",
   "metadata": {},
   "source": [
    "`.env`:\n",
    "```.env\n",
    "LANGCHAIN_API_KEY=ls__...\n",
    "LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_PROJECT=\"neuralbeagle-demo\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49da49db-f359-4703-99ae-909fd091f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LANGCHAIN_API_KEY',\n",
       " 'LANGCHAIN_ENDPOINT',\n",
       " 'LANGCHAIN_TRACING_V2',\n",
       " 'LANGCHAIN_PROJECT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for langsmith logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "[k for k in os.environ.keys() if 'langchain' in k.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb544e4-de85-4dc4-8c07-01a4f26ffccc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Call the model using `langchain_community.llms.LlamaCpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37ffd44-4e4c-4979-882b-3a1777fd702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013cc096-20b9-40a9-a4a4-c18dbff8ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.8 s\n",
      "Wall time: 2.82 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.name': 'mlabonne_neuralbeagle14-7b', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_path = r\"C:\\users\\joshua.bailey\\downloads\\neuralbeagle14-7b.Q5_K_M.gguf\"\n",
    "\n",
    "llm_kwargs = {\n",
    "    \"temperature\": 0.75,\n",
    "    \"max_tokens\": 5000,\n",
    "    \"top_p\": 1,\n",
    "    # The following settings are from https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "    # These settings used about 5.7GB GPU RAM on my system\n",
    "    \"n_gpu_layers\": 40,  # Change this value based on your model and your GPU VRAM pool.\n",
    "    \"n_batch\": 512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    # Callbacks support token-wise streaming\n",
    "    \"callback_manager\": CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    # Verbose is required to pass to the callback manager\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(model_path)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    **llm_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a109420-cc1b-4187-bc36-ac69f82f7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "llm_chain = PromptTemplate(template=template, input_variables=[\"question\"]) | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0ce5dd-4f11-4239-91c8-d676978ef77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are Scrub-Jays? Should a biologist expect to find them in Virginia?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ae770c-a826-468a-8dcb-be9e6641217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, let's consider what a Scrub-Jay is. There are two main types of Scrub-Jays found in North America: Western Scrub-Jay and the Eastern Scrub-Jay (also known as Blue Jay). The latter is also called the Florida Scrub-Jay because it has a limited range compared to the former, mostly restricted to peninsular Florida. This means that the Eastern or Florida Scrub-Jay would not be found in Virginia since its range does not extend there. On the other hand, the Western Scrub-Jay is widely distributed across the western United States and parts of Mexico. It's range includes California, Arizona, Nevada, Utah, Colorado, Oregon and Washington.\n",
      "\n",
      "However, Virginia is located on the east coast of the USA and is in close proximity to the Atlantic Ocean, which is outside the distribution range of Western Scrub-Jays. Nonetheless, there is a third kind of JAY that can be found in Virginia, and it's known as the Blue Jay (Cyanocitta cristata) - this is an eastern species and is quite distinct from the Eastern or Florida Scrub-Jay mentioned earlier.\n",
      "\n",
      "So, to summarize, a biologist should not expect to find Scrub-Jays (specifically, Western or Eastern Scrub-Jays) in Virginia. However, they would indeed find Blue Jays in that state.\n",
      "\n",
      "## Related Questions\n",
      "\n",
      "Below you may find additional questions related to the topic:\n",
      "\n",
      "### Is a scrub jay an endangered species?\n",
      "\n",
      "No, the Western Scrub-Jay is not considered an endangered species. However, as mentioned earlier, there is another type of Scrub-Jay referred to as the Eastern or Florida Scrub-Jay which has a more restricted distribution and is classified by IUCN Red List as Near Threatened due to its limited range and habitat loss.\n",
      "\n",
      "### Do scrub jays migrate?\n",
      "\n",
      "Both Western and Eastern Scrub-Jays are permanent residents in their respective ranges. The Western Scrub-Jay resides in the western parts of North America while the Eastern or Florida Scrub-CPU times: total: 13.9 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "answer = llm_chain.invoke(dict(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b59055-7276-4827-a437-90c320b152ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## View LangSmith run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8d1e6-4a5a-407f-ba8b-1ff5645afa4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "I've shared the resulting run [here](https://smith.langchain.com/public/0451496b-78a6-4cf1-b2e0-e58d6997d0ad/r).\n",
    "\n",
    "Time to first token: 207 ms\n",
    "\n",
    "Total tokens: 457 tokens\n",
    "\n",
    "Latency: 12.45 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## View logs in terminal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f9b3de7629f5768"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```text\n",
    "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
    "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
    "ggml_init_cublas: found 1 CUDA devices:\n",
    "  Device 0: NVIDIA RTX A4000 Laptop GPU, compute capability 8.6, VMM: yes\n",
    "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\\users\\joshua.bailey\\downloads\\neuralbeagle14-7b.Q5_K_M.gguf (version GGUF V3 (latest))\n",
    "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
    "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
    "llama_model_loader: - kv   1:                               general.name str              = mlabonne_neuralbeagle14-7b\n",
    "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
    "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
    "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
    "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
    "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
    "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
    "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
    "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
    "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
    "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
    "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
    "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
    "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
    "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
    "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
    "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
    "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
    "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
    "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
    "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
    "llama_model_loader: - type  f32:   65 tensors\n",
    "llama_model_loader: - type q5_K:  193 tensors\n",
    "llama_model_loader: - type q6_K:   33 tensors\n",
    "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
    "llm_load_print_meta: format           = GGUF V3 (latest)\n",
    "llm_load_print_meta: arch             = llama\n",
    "llm_load_print_meta: vocab type       = SPM\n",
    "llm_load_print_meta: n_vocab          = 32000\n",
    "llm_load_print_meta: n_merges         = 0\n",
    "llm_load_print_meta: n_ctx_train      = 32768\n",
    "llm_load_print_meta: n_embd           = 4096\n",
    "llm_load_print_meta: n_head           = 32\n",
    "llm_load_print_meta: n_head_kv        = 8\n",
    "llm_load_print_meta: n_layer          = 32\n",
    "llm_load_print_meta: n_rot            = 128\n",
    "llm_load_print_meta: n_embd_head_k    = 128\n",
    "llm_load_print_meta: n_embd_head_v    = 128\n",
    "llm_load_print_meta: n_gqa            = 4\n",
    "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
    "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
    "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
    "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
    "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
    "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
    "llm_load_print_meta: n_ff             = 14336\n",
    "llm_load_print_meta: n_expert         = 0\n",
    "llm_load_print_meta: n_expert_used    = 0\n",
    "llm_load_print_meta: rope scaling     = linear\n",
    "llm_load_print_meta: freq_base_train  = 10000.0\n",
    "llm_load_print_meta: freq_scale_train = 1\n",
    "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
    "llm_load_print_meta: rope_finetuned   = unknown\n",
    "llm_load_print_meta: model type       = 7B\n",
    "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
    "llm_load_print_meta: model params     = 7.24 B\n",
    "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW)\n",
    "llm_load_print_meta: general.name     = mlabonne_neuralbeagle14-7b\n",
    "llm_load_print_meta: BOS token        = 1 '<s>'\n",
    "llm_load_print_meta: EOS token        = 2 '</s>'\n",
    "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
    "llm_load_print_meta: PAD token        = 2 '</s>'\n",
    "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
    "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
    "llm_load_tensors: offloading 32 repeating layers to GPU\n",
    "llm_load_tensors: offloading non-repeating layers to GPU\n",
    "llm_load_tensors: offloaded 33/33 layers to GPU\n",
    "llm_load_tensors:        CPU buffer size =    85.94 MiB\n",
    "llm_load_tensors:      CUDA0 buffer size =  4807.05 MiB\n",
    "...................................................................................................\n",
    "llama_new_context_with_model: n_ctx      = 512\n",
    "llama_new_context_with_model: freq_base  = 10000.0\n",
    "llama_new_context_with_model: freq_scale = 1\n",
    "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
    "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
    "llama_new_context_with_model:  CUDA_Host input buffer size   =     9.01 MiB\n",
    "llama_new_context_with_model:      CUDA0 compute buffer size =    80.30 MiB\n",
    "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB\n",
    "llama_new_context_with_model: graph splits (measure): 3\n",
    "\n",
    "llama_print_timings:        load time =     126.88 ms\n",
    "llama_print_timings:      sample time =      60.32 ms /   464 runs   (    0.13 ms per token,  7692.56 tokens per second)\n",
    "llama_print_timings: prompt eval time =     126.79 ms /    48 tokens (    2.64 ms per token,   378.57 tokens per second)\n",
    "llama_print_timings:        eval time =   10946.64 ms /   463 runs   (   23.64 ms per token,    42.30 tokens per second)\n",
    "llama_print_timings:       total time =   12363.03 ms /   511 tokens\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "567d575896b4b301"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check `nvidia-smi` to see current GPU usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3502d40d330f10b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9435585-9b61-434e-9c1b-4f9fc0e22184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 28 17:30:04 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.79                 Driver Version: 537.79       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000 Laptop GPU  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   66C    P0              99W / 100W |   5705MiB /  8192MiB |     81%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     17084      C   ...\\conda\\envs\\neuralbeagle\\python.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
